{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16wBhcXL4XLnqSSVw7Pefj0beTbL6-vKN","timestamp":1736527084066},{"file_id":"1NLaaAx8uhYOZBeOceoyC_oZOSEUyxcXs","timestamp":1735750310985}],"toc_visible":true,"authorship_tag":"ABX9TyOvVA+zBHmO0rQ9Fks1S1LX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Unüberwachtes Lernen"],"metadata":{"id":"d2Qs0BwXHbUe"}},{"cell_type":"markdown","source":["## Lernziele\n","* Dimensionsreduktion mittels Hauptkomponentenanalyse zur Reduktion der Anzahl von Attributen in einem Datensatz anwenden können.\n","* Den K Means Algorithmus zum Finden von Clustern in Daten anwenden können.\n","* Die Optimale Anzahl von Clustern mittels dem Silhouettenkoeffizienten finden können."],"metadata":{"id":"gTE5eNugHd6e"}},{"cell_type":"markdown","source":["## Dimensionsreduktion\n"],"metadata":{"id":"VENRtAug7f1D"}},{"cell_type":"markdown","source":["In der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_3_merkmalskonstruktion_dimensionsreduktion/) haben wir Dimensionsreduktion als Werkzeug kennen gelernt, um dem [Fluch der Dimensionalität](https://janalasser.at/lectures/MC_KI/VO3_3_merkmalskonstruktion_dimensionsreduktion/#/2/2/3) zu entkommen. Das heißt, dass unser Datensatz *zu viele* verschiedene Attribute hat, um damit sinnvoll umgehen zu können. Insbesondere beim [unüberwachten Lernen](https://janalasser.at/lectures/MC_KI/VO3_1_unueberwachtes_lernen/) führt der Fluch der Dimensionalität dazu, dass die *Distanz* zwischen zwei Beobachtungen zunehmen an Bedeutung verliert, und Beobachtungen bzw. Gruppen von Beobachtungen (Cluster) schwerer voneinander zu unterscheiden sind.  \n","\n","\n","Im Folgenden wollen wir die Hauptkomponentenanalyse als Mittel zur Dimensionsduktion in Python umsetzen. Dazu verwenden wir wieder den Brustkrebs-Datensatz aus dem letzten Kurs."],"metadata":{"id":"vXTFJBawo5tC"}},{"cell_type":"code","source":["# lade Brustkrebsdaten\n","from sklearn import datasets\n","brustkrebs = datasets.load_breast_cancer(as_frame=True)\n","\n","# teile Daten in Attribute und Zielwerte\n","x_brustkrebs = brustkrebs[\"data\"]\n","y_brustkrebs = brustkrebs[\"target\"]\n","\n","x_brustkrebs.head()"],"metadata":{"id":"zahi5WJMInWZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Als erstes möchten wir uns ansehen, ob sich die Attribute von Beobachtungen mit Krebs systematisch von denen von Beobachtungen ohne Krebs unterscheiden. Dafür visualisieren wir die Daten mit Histogrammen und Punktwolken."],"metadata":{"id":"3uDT4lYD-YRC"}},{"cell_type":"code","source":["import seaborn as sns\n","# da unser Datensatz 30 Attribute (Spalten) hat, beschränken wir uns zu\n","# Illustrationszwecken auf die ersten vier Attribute\n","interessante_spalten = [\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\"]\n","\n","# um die Daten einfach mit seaborn darstellen zu können, erstellen wir uns ein\n","# DataFrame, das die relevanten Attribute sowie die Zielwerte enthält\n","plot_daten_brustkrebs = x_brustkrebs[interessante_spalten].copy()\n","plot_daten_brustkrebs[\"target\"] = y_brustkrebs.copy()"],"metadata":{"id":"4CZTfA_W2740"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`seaborn` bietet die praktische Funktion `PairGrid()` (siehe [Dokumentation](https://seaborn.pydata.org/generated/seaborn.PairGrid.html)). Sie erzeugt ein Raster von Abbildungen mit jeweils so vielen Zeilen und Spalten wie Attribute im Datensatz enthalten sind.\n","* Auf der **Diagonalen** des Rasters wird je Attribut ein Histogramm der Attributswerte dargestellt.\n","* **Neben der Diagonalen** wird für jeweils zwei Attribute eine Punktwolke dargestellt.  \n","\n","Insgesamt gibt es so für jede mögliche Kombination von zwei Attributen eine Punktwolke.  "],"metadata":{"id":"xMjw9VlBp6ke"}},{"cell_type":"code","source":["# wir Erzeugen ein Raster von Abbildungen\n","g = sns.PairGrid(plot_daten_brustkrebs, hue=\"target\")\n","# wir legen fest, dass auf der Diagonalen Histogramme zu sehen sein sollen\n","g.map_diag(sns.histplot)\n","# wir legen fest, dass neben der Diagonalen Punktwolken zu sehen sein sollen\n","g.map_offdiag(sns.scatterplot, alpha=0.5)\n","# wir fügen manuell eine Legende hinzu, da diese nicht automatisch erzeugt wird\n","g.add_legend();"],"metadata":{"id":"mDTf72lyg7DP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir sehen, dass die Punktwolken für Beobachtungen mit Krebs und solchen ohne Krebs sich durchaus unterscheiden - aber es gibt auch einen relativ größen Überlapp zwischen den Bereichen. Können wir die Punktwolken besser voneinander separieren?   \n","\n","Dabei kann uns die Hauptkomponentenanalyse (principal component analysis, PCA) helfen. Wie unten illustriert, ist die Idee einer PCA, die einzelnen Dimensionen des Datensatz auf neue Dimensionen zu *projizieren*, die es einfacher machen, die Punktwolken voneinander zu unterscheiden."],"metadata":{"id":"CXYuX67H7W7L"}},{"cell_type":"code","source":["from IPython.display import Image\n","Image(url='https://upload.wikimedia.org/wikipedia/commons/9/9c/PCA_Projection_Illustration.gif')"],"metadata":{"id":"pKcRTcG8rYpk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_3_merkmalskonstruktion_dimensionsreduktion/#/2) haben wir die PCA schon als Werkzeug zur Dimensionsreduktion kennen gelernt. Auch für PCA bietet `scikit-learn` Funktionen, die teilweise sehr ähnlich zu denen funktionieren, die wir schon beim überwachten Lernen kennen gelernt haben."],"metadata":{"id":"Jq9VnvwP8cC7"}},{"cell_type":"code","source":["# wir importieren das PCA \"Modell\" aus scikit-learn\n","from sklearn.decomposition import PCA\n","\n","# wir instantiieren ein leeres Modell und legen die Hyperparameter fest. Da wir\n","# nur die ersten beiden Hauptkomponenten (principal components) der Daten\n","# behalten wollen, legen wir n_components=2 fest\n","pca_brustkrebs = PCA(n_components=2)\n","\n","# schlussendlich trainieren (fitten) wir das Modell auf den Brustkrebsdaten\n","pca_brustkrebs.fit(x_brustkrebs)"],"metadata":{"id":"ImlaqPU08plS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Im weiteren Vorgehen unterscheided sich die Anwendung von `pca()` aber von der des überwachten Lernens: wir wollen keine Werte für einzelne Beobachtungen vorhersagen sondern den gesamten Datensatz \"transformieren\". Deswegen verwenden wir die `transform()` Funktion des trainierten Modells."],"metadata":{"id":"gRxYv1LX9NqV"}},{"cell_type":"code","source":["# wir transformieren die Daten indem wir sie auf die ersten beiden\n","# Hauptkomponenten projizieren. Übrig bleibt ein Datensatz, der nicht mehr 30\n","# Dimensionen hat sondern nur noch 2\n","x_brustkrebs_pca = pca_brustkrebs.transform(x_brustkrebs)\n","print(\"Originale Dimensionen des Datensatzes: {}\".format(str(x_brustkrebs.shape)))\n","print(\"Reduzierte Dimensionen des Datensatzes: {}\".format(str(x_brustkrebs_pca.shape)))"],"metadata":{"id":"pLgrA6Sniiwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# leider gehen beim Transformationsprozess die Spaltennamen verloren\n","x_brustkrebs_pca"],"metadata":{"id":"I7h0H2A_oNbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# wir erzeugen deshalb wieder ein pandas DataFrame aus den transformierten\n","# Daten und fügen manuell die Spaltennamen hinzu\n","import pandas as pd\n","x_brustkrebs_pca = pd.DataFrame(data=x_brustkrebs_pca, columns=[\"PC1\", \"PC2\"])\n","x_brustkrebs_pca.head()"],"metadata":{"id":"v_s7F7P8iy4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# um die Daten zu visualisieren, fügen wir noch die Spalte \"target\" mit den\n","# Zielwerten aus den originalen (nicht transformierten) Daten hinzu\n","x_brustkrebs_pca[\"target\"] = y_brustkrebs.copy()\n","\n","# und stellen die auf die ersten beiden Hauptkomponenten projizierten Daten\n","# in einer Punktwolke dar\n","sns.scatterplot(data=x_brustkrebs_pca, x=\"PC1\", y=\"PC2\", hue=\"target\", alpha=0.3)"],"metadata":{"id":"7rT7GZdYoaUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Die Idee hinter der Hauptkomponentenanalyse ist, möglichst viel der im Datensatz enthaltenen *Varianz* in wenige Hauptkomponenten zu verschieben und damit effektiv die Dimensionen des Datensatzes zu reduzieren. Wie viel Varianz wird von den Hauptkomponenten jeweils erklärt? Diese Information ist in dem Attribut `explained_variance_ratio_` des \"trainierten\" PCA-Modells enthalten:"],"metadata":{"id":"0RCxcVSFLaD0"}},{"cell_type":"code","source":["pca_brustkrebs.explained_variance_ratio_"],"metadata":{"id":"NVnElgKoMImt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# wir können das auch grafisch mit einem Balkendiagramm darstellen. Dafür\n","# basteln wir uns zuerst ein passendes DataFrame ...\n","erklaerte_varianz = pd.DataFrame()\n","erklaerte_varianz[\"komponente\"] = range(1, len(pca_brustkrebs.explained_variance_ratio_) + 1)\n","erklaerte_varianz[\"erklaerte varianz\"] = pca_brustkrebs.explained_variance_ratio_\n","\n","# ... und verwenden dann die barplot() Funktion von seaborn\n","sns.barplot(data=erklaerte_varianz, x=\"komponente\", y=\"erklaerte varianz\")"],"metadata":{"id":"y9Mr9-z4BeBL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Jede der Hauptkomponenten besteht aus einer Mischung (Linearkombination) der original im Datensatz enthaltenen Attribute. Wir können uns auch ansehen, welche Attribute für welche der Hauptkomponenten besonders wichtig sind. Diese Information ist im Attribut `components_` des \"trainierten\" PCA-Modells enthalten. Wobei wir hier über den Index auf die verschiedenen Hauptkomponenten zugreifen können. `components_[0]` z.B. enthält die Information darüber, wie wichtig die Attribute des Datensatzes jeweils für die erste Hauptkomponente sind, `components_[1]` für die zweite, usw."],"metadata":{"id":"Y0IGzYkGMej-"}},{"cell_type":"code","source":["# auch die wichtigkeit der Attribute für die Hauptkomponenten können wir\n","# grafisch darstellen und basteln uns dafür ein pandas DataFrame.\n","wichtigkeit_attribute_brustkrebs = pd.DataFrame()\n","wichtigkeit_attribute_brustkrebs['attribut'] = x_brustkrebs.columns\n","wichtigkeit_attribute_brustkrebs['PC1'] = pca_brustkrebs.components_[0]\n","wichtigkeit_attribute_brustkrebs['PC2'] = pca_brustkrebs.components_[1]"],"metadata":{"id":"jTEQp5Ju_dWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wichtigkeit der Attribute in der ersten Hauptkomponente\n","sns.barplot(data=wichtigkeit_attribute_brustkrebs, y='attribut', x='PC1')"],"metadata":{"id":"zuXJTiaQ_96u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wichtigkeit der Attribute in der zweiten Hauptkomponente\n","sns.barplot(data=wichtigkeit_attribute_brustkrebs, y='attribut', x='PC2')"],"metadata":{"id":"X99h4odcBMbL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exkurs: Datenskalierung"],"metadata":{"id":"JdS3MhHUmYn-"}},{"cell_type":"markdown","source":["Bis jetzt aben wir die Daten \"as is\" verwendet. Aber auch bei der PCA (und später beim unüberwachten Lernen bzw. Clustering) kann es nützlich sein, die Daten zuerst zu transformieren, um sie in einen ähnlichen Wertebereich zu bringen.  \n","\n","Im Folgenden machen wir das einmal \"zu Fuß\" für das Attribut `mean radius` des Datensatzes:"],"metadata":{"id":"agiG541uNpsV"}},{"cell_type":"code","source":["x_brustkrebs[\"mean radius\"].head()"],"metadata":{"id":"2MePqPBsmiCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# so sieht das Attribut aus, bevor wir es transformiert haben\n","sns.histplot(x_brustkrebs, x=\"mean radius\")"],"metadata":{"id":"BfIISoOvmzEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# als ersten Schritt \"zentrieren\" wir das Attribut, indem wir den Mittelwert\n","# aller Beobachtungen abziehen\n","x_brustkrebs[\"mean radius zentriert\"] = x_brustkrebs[\"mean radius\"] - x_brustkrebs[\"mean radius\"].mean()\n","x_brustkrebs[\"mean radius zentriert\"].head()"],"metadata":{"id":"Z8Ukm4tgmloG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# die Form der Verteilung ändert sich dadurch nicht, nur ihr Zentrum\n","sns.histplot(x_brustkrebs, x=\"mean radius zentriert\")"],"metadata":{"id":"QRJJYN0gm4X2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# als zweiten Schritt skalieren wir die Daten, indem wir sie durch die\n","# Standardabweichung aller Beobachtungen teilen\n","x_brustkrebs[\"mean radius skaliert\"] = x_brustkrebs[\"mean radius zentriert\"] / x_brustkrebs[\"mean radius\"].std()\n","x_brustkrebs[\"mean radius skaliert\"].head()"],"metadata":{"id":"sDYWKvOtm750"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# auch hier verändert sich die Form der Verteilung nicht, nur der von ihr\n","# abgedeckte Wertebereich\n","sns.histplot(x_brustkrebs, x=\"mean radius skaliert\")"],"metadata":{"id":"uOjYnhZ-nFMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir müssen das Transformieren der Daten aber nicht \"von Hand\" erledigen - auch dafür liefert `scikit-learn` eingebaute Funktionalität. Die Transformationen die wir gerade durchgeführt haben (zentrieren, skalieren mit der Standardabweichung) werden vom `StandardScaler()` automatisch für alle Attribute (Spalten) des Datensatzes durchgeführt:"],"metadata":{"id":"Gf6mZamIOprD"}},{"cell_type":"code","source":["# zuerst entfernen wir die beiden Spalten, die wir gerade manuell hinzugefügt\n","# haben wieder, da wir sie im späteren Verlauf nicht mehr brauchen\n","x_brustkrebs = x_brustkrebs.drop(columns=[\"mean radius zentriert\", \"mean radius skaliert\"])"],"metadata":{"id":"5PRYCvkaP7fv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# wir importieren den StandardScaler aus scikit-learn\n","from sklearn.preprocessing import StandardScaler\n","\n","# wir erstellen einen \"leeren\" Skalierer ...\n","skalierer_brustkrebs = StandardScaler()\n","\n","# ... und \"trainieren\" ihn auf dem Datensatz. Im Fall des StandardScaler\n","# passiert beim \"Training\" eigentlich nichts, außer dass der Funktion die Daten\n","# übergeben werden\n","skalierer_brustkrebs.fit(x_brustkrebs)\n","\n","# wir skalieren den Daten mit der transform()-Funktion des Skalierers.\n","# die Funktion gibt uns die skalierten Daten zurück und wir speichern sie\n","# in einer neuen Variablen\n","x_brustkrebs_skaliert = skalierer_brustkrebs.transform(x_brustkrebs)\n","\n","# leider gehen auch hier wieder die Spaltennamen verloren und wir fügen sie\n","# wieder manuell hinzu\n","x_brustkrebs_skaliert = pd.DataFrame(x_brustkrebs_skaliert, columns=x_brustkrebs.columns)\n","x_brustkrebs_skaliert.head()"],"metadata":{"id":"rIeJaNvhlcZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nun wollen wir erkunden, ob durch die Transformation der Daten die PCA besser in der Lage ist, die beiden Klassen (\"krebs\", \"kein krebs\") zu separieren."],"metadata":{"id":"KCJ2P94-QHAf"}},{"cell_type":"code","source":["# wir instantiieren ein neues PCA modell und fitten es mit den skalierten\n","# Brustkrebsdaten\n","pca_brustkrebs_skaliert = PCA(n_components=2)\n","pca_brustkrebs_skaliert.fit(x_brustkrebs_skaliert)\n","\n","# wir projizieren die skalierten Brustkrebsdaten auf die ersten beiden\n","# Hauptkomponenten\n","x_brustkrebs_pca_skaliert = pca_brustkrebs_skaliert.transform(x_brustkrebs_skaliert)\n","\n","# auch hier sind wieder die Spaltennamen verlorengegangen und wir fügen sie\n","# hinzu, bevor wir die Daten visualisieren\n","x_brustkrebs_pca_skaliert = pd.DataFrame(data=x_brustkrebs_pca_skaliert, columns=[\"PC1\", \"PC2\"])\n","x_brustkrebs_pca_skaliert[\"target\"] = y_brustkrebs\n","\n","# Visualisierung mit Punktwolke\n","sns.scatterplot(data=x_brustkrebs_pca_skaliert, x=\"PC1\", y=\"PC2\", hue=\"target\", alpha=0.5)"],"metadata":{"id":"h9368_zikCT2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# die erklärte Varianz verteilt sich jetzt gleichmäßiger auf die beiden\n","# Hauptkomponenten\n","erklaerte_varianz_skaliert = pd.DataFrame()\n","erklaerte_varianz_skaliert[\"komponente\"] = range(1, len(pca_brustkrebs_skaliert.explained_variance_ratio_) + 1)\n","erklaerte_varianz_skaliert[\"erklaerte varianz\"] = pca_brustkrebs_skaliert.explained_variance_ratio_\n","sns.barplot(data=erklaerte_varianz_skaliert, x=\"komponente\", y=\"erklaerte varianz\")"],"metadata":{"id":"qD3zgfX5B8J_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# und es fließen mehr Attribute in die Hauptkomponenten ein\n","wichtigkeit_attribute_brustkrebs_skaliert = pd.DataFrame()\n","wichtigkeit_attribute_brustkrebs_skaliert['attribut'] = x_brustkrebs_skaliert.columns\n","wichtigkeit_attribute_brustkrebs_skaliert['PC1'] = pca_brustkrebs_skaliert.components_[0]\n","wichtigkeit_attribute_brustkrebs_skaliert['PC2'] = pca_brustkrebs_skaliert.components_[1]\n","\n","sns.barplot(data=wichtigkeit_attribute_brustkrebs_skaliert, y='attribut', x='PC1')"],"metadata":{"id":"C4oqeSUCATMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.barplot(data=wichtigkeit_attribute_brustkrebs_skaliert, y='attribut', x='PC2')"],"metadata":{"id":"xvieG9dsBGSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Übung 1"],"metadata":{"id":"FFFYJYvuG-8L"}},{"cell_type":"markdown","source":["<font color='blue'>In der Code-Zelle unten wird ein Datensatz von Merkmalen von Blumen geladen, die verschiedenen Spezies angehören. Bei Interesse können Sie mehr Informationen zu dem Datensatz [hier](https://en.wikipedia.org/wiki/Iris_flower_data_set) nachlesen.\n","<ul class=\"outside\">\n","<li><font color='blue'>Machen Sie sich mit dem Datensatz vertraut: wie viele Beobachtungen gibt es? Wie viele Merkmale und was bedeuten sie?</font></li>\n","<li><font color='blue'>Skalieren Sie die Daten indem Sie von jedem Attribut den Mittelwert abziehen und durch die Standardabweichung teilen. Hinweis: `StandardScaler()` von `scikit-learn` benutzen!</font></li>\n","<li><font color='blue'>Führen Sie eine Hauptkomponentenanalyse (PCA) mit `n_components=2` durch. Wie viel Varianz bilded die erste Hauptkomponente (PC 1) ab, wie viel die zweite (PC 2)?</font></li>\n","<li><font color='blue'>Welche Attribute des Datensatzes sind für die erste Hauptkomponente (PC 1) wichtig, welche für die zweite (PC 2)?</font></li>\n","</ul>\n","<font color='blue'></font>"],"metadata":{"id":"0a_c8ME2CrVn"}},{"cell_type":"code","source":["iris = datasets.load_iris(as_frame=True)\n","x_iris = iris[\"data\"]\n","y_iris = iris[\"target\"]\n","\n","plot_daten_iris = x_iris.copy()\n","plot_daten_iris[\"target\"] = y_iris.copy()\n","\n","g = sns.PairGrid(plot_daten_iris, hue=\"target\")\n","g.map_diag(sns.histplot)\n","g.map_offdiag(sns.scatterplot, alpha=0.5)\n","g.add_legend();"],"metadata":{"id":"CZ_ZrAuGDAaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Skalierung: Ihr Code hier"],"metadata":{"id":"sypZeM_MSWNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PCA: ihr Code hier"],"metadata":{"id":"um9pw6YSSYxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anteil der erklärten Varianz: Ihr Code hier"],"metadata":{"id":"T8doRFbqScGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wichtigkeit der Attribute: Ihr Code hier"],"metadata":{"id":"zimy35mYSgNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering mit K-Means\n","Bis jetzt haben wir zu Illustrationszwecken immer noch die Klassenzugehörigkeit (\"krebs\", \"kein krebs\" bei den Brustkrebsdaten bzw. \"setosa\", \"versicolor\" und \"virginica\" bei den Blumen) verwendet. Wie gehen wir aber vor, wenn wir die Klassenzugehörigkeit nicht kennen sondern herausfinden wollen? Das ist der klassische Anwendungsfall für unüberwachtes Lernen bzw. [Clustering](https://janalasser.at/lectures/MC_KI/VO3_1_unueberwachtes_lernen/).  \n","\n","Um das Konzept zu illustrieren, verwenden wir einen \"echten\" Datensatz von Merkmalen von Bauernhöfen, den wir auch in der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_1_unueberwachtes_lernen/#/1/2) schon kurz besprochen haben. Zu diesem Datensatz gibt es auch ein Forschungsprojekt – bei Interesse können Sie Details in dieser [Publikation](https://www.nature.com/articles/s41598-021-00469-2) nachlesen. Unser Ziel wird es sein herauszufinden, ob die Bauernhöfe sich anhand ihrer Merkmale in unterschiedliche Gruppen aufteilen lassen.\n"],"metadata":{"id":"2-pxrH4i4gy_"}},{"cell_type":"code","source":["# wir laden den Datensatz mit Merkmalen von Bauernhöfen in ein pandas DataFrame\n","# er hat 160 Attribute aber keine Zielwerte\n","url = \"https://drive.google.com/uc?id=1mNO7yf89ReYPvjJgfD3YdY5MdIVGvCtp\"\n","farm = pd.read_csv(url)\n","farm.head()"],"metadata":{"id":"BQ0kEkgTN9TQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# wie für die Brustkrebsdaten skalieren wir die Daten ...\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","scaler.fit(farm)\n","farm_skaliert = scaler.transform(farm)\n","farm_skaliert = pd.DataFrame(farm_skaliert, columns=farm.columns)"],"metadata":{"id":"Nqis-5EqPbxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ... und führen eine PCA zur Dimensionsreduktion durch\n","from sklearn.decomposition import PCA\n","pca_farm = PCA(n_components=2)\n","pca_farm.fit(farm_skaliert)\n","farm_pca = pca_farm.transform(farm_skaliert)\n","farm_pca = pd.DataFrame(data=farm_pca, columns=[\"PC1\", \"PC2\"])\n","sns.scatterplot(data=farm_pca, x=\"PC1\", y=\"PC2\", alpha=0.3)"],"metadata":{"id":"jiKA5XnYPrKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir wollen nun, wie in der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_1_unueberwachtes_lernen/#/0/3/8) beschrieben, eine Clusteranalyse durchführen, um verschiedene Cluster zu finden (die wir dann als \"Klassen\" interpretieren können). In der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_4_ubueberwachtes_lernen_algorithmen/#/4) haben wir dafür den K-Means Algorithmus kennengelernt. Er findet Cluster, indem er ausgehend von einem bestehenden (zufälligen) Clustering am Anfang Beobachtungen in andere Cluster verschiebt, bis die Aufteilung der Beobachtungen in Cluster optimal ist."],"metadata":{"id":"vfrPEcwZHinX"}},{"cell_type":"code","source":["from IPython.display import Image\n","Image(url='https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif')"],"metadata":{"id":"UAsDXQLcIOLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Auch für den K means Algorithmus bietet `scikit-learn` eine vorgefertigte Lösung die wir nach dem üblichen Schema verwenden können. Auch der K means Algorithmus hat Hyperparameter:\n","* `n_clusters`: wir müssen im Vorhinein festlegen, wie viele Cluster wir erwarten. Für die vorliegenden Daten zu Bauernhöfen wissen wir nicht wirklich, wie viele Cluster wir erwarten sollen. Wir können in diesem Fall nur raten, bzw. die Anzahl der Cluster systematisch durchprobieren.\n","* `random_state`: da das Clustering eine Zufallskomponente beinhaltet (welchem Cluster die Beobachtungen zu Beginn zugeordnet sind) ist es gut, den Zufall \"festzuhalten\" um Ergebnisse reproduzierbar zu machen."],"metadata":{"id":"Ybuu0aGVIbPx"}},{"cell_type":"code","source":["# importiere das KMeans Modell\n","from sklearn.cluster import KMeans\n","\n","# instantiiere ein leeres Modell und lege die Hyperparameter fest.\n","# wir entscheiden uns im ersten Versuch für drei vorgegebene Cluster\n","kmeans_farm = KMeans(n_clusters=3, random_state=42)\n","\n","# trainiere den Algorithmus auf den ersten beiden Hauptkomponenten der\n","# Brustkrebsdaten\n","kmeans_farm.fit(farm_pca)\n","\n","# sage die Clusterzugehörigkeit der Beobachtungen voraus\n","cluster_farm = kmeans_farm.predict(farm_pca)"],"metadata":{"id":"6nDi_Zj42huH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_farm"],"metadata":{"id":"l734TXyfJUF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# um die Clusterzugehörigkeit zu visualisieren, erstellen wir ein DataFrame\n","# mit den Hauptkomponenten und der vorhergesagten Clusterzugehörigkeit\n","plot_daten_farm = farm_pca.copy()\n","plot_daten_farm[\"cluster\"] = cluster_farm"],"metadata":{"id":"XokQrH9ESxWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.scatterplot(data=plot_daten_farm, x=\"PC1\", y=\"PC2\", hue=\"cluster\", alpha=0.5, palette=sns.color_palette(\"tab10\"))"],"metadata":{"id":"QjUduGSrKO7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wie in der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_4_ubueberwachtes_lernen_algorithmen/#/4/2/1) besprochen, hängt das Ergebnis des K Means Algorithmus vom gewählten Ausgangsclustering ab. Das Ausgangsclustering wird zufällig festgelegt – indem wir `random_state` auf eine Zahl setzen stellen wir sicher, dass es immer gleich gewählt wird. Was passiert, wenn wir einen anderen `random_state` setzen und damit ein anderes Ausgangsclustering wählen?"],"metadata":{"id":"9pldvLtQsVrH"}},{"cell_type":"code","source":["# wir setzen den random_state auf 11 (war vorher 42) und lassen den Algorithmus\n","# nochmal laufen\n","kmeans_farm = KMeans(n_clusters=3, random_state=11)\n","kmeans_farm.fit(farm_pca)\n","\n","# sage die Clusterzugehörigkeit der Beobachtungen voraus\n","cluster_farm_neuer_random_state = kmeans_farm.predict(farm_pca)\n","\n","# Visualisierung der Daten mit vier Clustern\n","plot_daten_farm[\"cluster_4_rnd\"] = cluster_farm_neuer_random_state\n","sns.scatterplot(data=plot_daten_farm, x=\"PC1\", y=\"PC2\", hue=\"cluster_4_rnd\", alpha=0.5, palette=sns.color_palette(\"tab10\"));"],"metadata":{"id":"061pULXksDy1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Was passiert, wenn wir die Anzahl der Cluster auf `n_clusters=4` setzen?"],"metadata":{"id":"rPNp1iqVTrIt"}},{"cell_type":"code","source":["# instantiiere ein leeres Modell und lege die Hyperparameter fest.\n","kmeans_farm = KMeans(n_clusters=4, random_state=11)\n","\n","# trainiere den Algorithmus auf den ersten beiden Hauptkomponenten der\n","# Brustkrebsdaten\n","kmeans_farm.fit(farm_pca)\n","\n","# sage die Clusterzugehörigkeit der Beobachtungen voraus\n","cluster_farm_4 = kmeans_farm.predict(farm_pca)"],"metadata":{"id":"8YEhJTKCT1Ys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_farm_4"],"metadata":{"id":"ncdHr3fxUWMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualisierung der Daten mit vier Clustern\n","plot_daten_farm[\"cluster_4\"] = cluster_farm_4\n","sns.scatterplot(data=plot_daten_farm, x=\"PC1\", y=\"PC2\", hue=\"cluster_4\", alpha=0.5, palette=sns.color_palette(\"tab10\"));"],"metadata":{"id":"eD-NPm5-UZlI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Die optimale Anzahl von Clustern finden"],"metadata":{"id":"Wcs75SwFtYWI"}},{"cell_type":"markdown","source":["Wie können wir die beste Anzahl an Clustern herausfinden? Wir können versuchen, die Qualität des Clustering zu messen. Hierfür brauchen wir ein Maß bzw. eine Metrik für die Qualität des Clustering.  \n","\n","In der [Vorlesung](https://janalasser.at/lectures/MC_KI/VO3_4_ubueberwachtes_lernen_algorithmen/#/4/3/3) haben wir den Silhouettenkoeffizient als möglichkeit zur Messung der Qualität eines Clusterings besprochen. Er misst, wie ähnlich eine Beobachtung anderen Beobachtungen im selben Cluster ist (Kohäsion) verglichen mit Beobachtungen in anderen Clustern (Separierung).\n","\n","![silhouette score](https://drive.google.com/uc?id=17x-xSwIN5QUaB_IJ_Uw8IAgxAz7Pv0eF)\n","\n","Auch für den Silhouettenkoeffizient gibt es in scikit-learn eine Funktion `silhouette_score()` im `metrics` Modul, die wir einfach verwenden können (siehe auch [Dokumentation](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.silhouette_score.html)).  \n","\n","Faustregel: Ein Clustering mit einem Silhouettenkoeffizienten von >= 0.7 ist ein \"starkes\" Clustering, ein Silhouettenkoeffizient von >= 0.5 ist ein \"befriedigendes\" Clustering, und ein Silhouettenkoeffizient von >= 0.25 ist ein \"schwaches\" Clustering. Werte darunter deuten darauf hin, dass es keine Cluster-Struktur in den Daten gibt. Insbesondere bei hochdimensionalen Daten ist bei der Interpretation aber Achtung geboten, da der [Fluch der Dimensionalität](https://janalasser.at/lectures/MC_KI/VO3_3_merkmalskonstruktion_dimensionsreduktion/#/2/2/3) dafür sorgt, dass die Werte des Silhouettenkoeffizienten insgesamt kleiner werden.\n","\n","Alternativ können wir auch einen Clustering-Algorithmus wählen, der selbstständig die beste Anzahl von Clustern findet, wie z.B. DBSCAN (siehe Übung)."],"metadata":{"id":"aYdgW4qUtdgg"}},{"cell_type":"code","source":["# wir messen den Silhouettenkoeffizienten des Clusterings der Bauernhöfe\n","# mit 3 bzw. 4 vorgegebenen Clustern:\n","from sklearn.metrics import silhouette_score\n","sil_score_3 = silhouette_score(farm_pca, cluster_farm)\n","sil_score_4 = silhouette_score(farm_pca, cluster_farm_4)\n","\n","print(f\"Silhouette Score mit n_clusters=3: {sil_score_3}\")\n","print(f\"Silhouette Score mit n_clusters=4: {sil_score_4}\")"],"metadata":{"id":"Gn7jCugRrCMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Übung 2\n","<ul class=\"outside\">\n","<li><font color='blue'>Führen Sie das K Means Clustering für den \"Iris\"-Datensatz durch. Wählen Sie dafür einmal <tt>n_clusters=3</tt> und einmal <tt>n_clusters=2</tt>. Setzen Sie für beide Fälle den random state auf <tt>random_state=42</tt>. Speichern Sie die Ergebnisse jeweils in separaten Variablen.</font></li>\n","<li><font color='blue'>Visualisieren Sie das Clustering für beide Fälle mit Streudiagrammen wie schon für den Bauernhof-Datensatz.</font></li>\n","<li><font color='blue'>Berechnen Sie den Silhouettenkoeffizient für beide Fälle. Für welche Clusteranzahl ist er besser?</font></li>"],"metadata":{"id":"JoVvnK40LkFi"}},{"cell_type":"markdown","source":["## Hausaufgaben\n","\n","Die Hausaufgaben für diesen Kursteil finden sich in [diesem Notebook](https://colab.research.google.com/drive/1sdDyGrwca9UIqw1QxwLF8Zk-_AI9NkeI?usp=sharing)."],"metadata":{"id":"bUMdiMZNchjI"}},{"cell_type":"markdown","source":["## Weiterführende Materialien\n","* **machine learning**: [Buch](https://www.amazon.de/Introduction-Machine-Learning-Python-Scientists/dp/1449369413?shipTo=AT&source=ps-sl-shoppingads-lpcontext&ref_=fplfs&psc=1&smid=A3JWKAKR8XB7XF&language=de_DE&gQT=1) Introduction to Machine Learning with Python: A Guide for Data Scientists mit umfassenden [Beispielen und Übungen](https://github.com/amueller/introduction_to_ml_with_python) in Python."],"metadata":{"id":"1s3FxpXzmSA6"}},{"cell_type":"markdown","source":["## Quelle und Lizenz\n","\n","Das vorliegende Notebook wurde von Jana Lasser für den Kurs B \"technische Aspekte\" des Microcredentials \"KI und Gesellschaft\" der Universität Graz erstellt.\n","\n","Das Notebook kann unter den Bedingungen der Lizenz [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0) verwendet, modifiziert und weiterverbreitet werden.\n"],"metadata":{"id":"HF-P2SdOEhCg"}}]}